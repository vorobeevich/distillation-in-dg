{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "sys.path.append(\"../\")\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from tqdm import tqdm \n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from src.utils import init_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy function\n",
    "def check(model, loader):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        accuracy = 0\n",
    "        for batch in loader:\n",
    "            images, labels = batch[\"image\"], batch[\"label\"]\n",
    "            images, labels = images.to(\"cuda\").float(), labels.to(\"cuda\").long()\n",
    "            logits = model(images)\n",
    "            ids = F.softmax(logits, dim=-1).argmax(dim=-1)\n",
    "            batch_true = (ids == labels).sum()\n",
    "            accuracy += batch_true.item()\n",
    "        return accuracy / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PACS(torch.utils.data.Dataset):\n",
    "    \"\"\"Class with standard methods for torch dataset for working with PACS dataset.\n",
    "    Inherited from standard class torch.utils.data.Dataset.\n",
    "    Dataset paper: https://arxiv.org/abs/1710.03077.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_type: list[str],\n",
    "            domain_list: list[str],\n",
    "            transforms: torchvision.transforms.Compose,\n",
    "            augmentations: torchvision.transforms.Compose = None) -> None:\n",
    "        \"\"\"Dataset initialization. Creates images list (where file paths are stored)\n",
    "        and classes (labels) torch.Tensor for them.\n",
    "\n",
    "        Args:\n",
    "            dataset_types (list[str]): list of values from {'train', 'test'}.\n",
    "            domain (list[str]): list of values from {'art_painting', 'cartoon', 'photo', 'sketch'}.\n",
    "            transforms (torchvision.transforms.Compose): transforms that are applied to each\n",
    "                image (regardless of whether it is in train or test selection).\n",
    "            augmentations (torchvision.transforms.Compose,\n",
    "            optional): augmentations that apply only to the train selection. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.images = []\n",
    "        self.labels = torch.Tensor([])\n",
    "        self.domain_list = domain_list\n",
    "        for domain in domain_list:\n",
    "            imgs, lbls = self.get_paths_and_labels(dataset_type, domain)\n",
    "            self.images += imgs\n",
    "            self.labels = torch.cat((self.labels, lbls))\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def get_paths_and_labels(self,\n",
    "                             dataset_types: list[str],\n",
    "                             domain: str) -> tuple[list[str],\n",
    "                                                   torch.Tensor]:\n",
    "        \"\"\"Return list of images paths for a given type of the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_types (list[str]): list of values from {'train', 'test'}.\n",
    "            domain (str): one of 'art_painting', 'cartoon', 'photo', 'sketch'.\n",
    "\n",
    "        Returns:\n",
    "            tuple[list[str], torch.Tensor]: paths to images and tensor with class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        paths = []\n",
    "        labels = []\n",
    "        for ds_type in dataset_types:\n",
    "            filepath = f\"data/pacs/labels/{domain}_{ds_type}.txt\"\n",
    "            f = open(filepath, 'r')\n",
    "            lines = f.readlines()\n",
    "            f.close()\n",
    "            lines = [l.split() for l in lines]\n",
    "            cur_paths, cur_labels = zip(*lines)\n",
    "            cur_labels = [int(l) for l in cur_labels]\n",
    "            paths += cur_paths\n",
    "            labels += cur_labels\n",
    "        global inds\n",
    "        inds = np.zeros(len(paths))        \n",
    "        return paths, torch.Tensor(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: dataset len\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Returns a picture from the dataset by its number.\n",
    "        First, the image is read along the path, augmentations are applied to it (if necessary), then transforms.\n",
    "        Also, the class label is returned.\n",
    "\n",
    "        Args:\n",
    "            idx (int): index of image\n",
    "        Returns:\n",
    "            dict[str, torch.Tensor]: dict:\n",
    "                {\n",
    "                    \"image\": image torch.Tensor,\n",
    "                    \"label\": label torch.Tensor\n",
    "                }\n",
    "        \"\"\"\n",
    "        img_name = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.augmentations:\n",
    "            sample = {\n",
    "                'image':\n",
    "                self.augmentations(image)\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "            }\n",
    "\n",
    "        sample['image'] = self.transforms(sample['image'])\n",
    "        sample['label'] = label\n",
    "        inds[idx] += 1\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset config\n",
    "dataset = {\n",
    "    \"name\": \"PACS\",\n",
    "    \"kwargs\": {\n",
    "        \"domain_list\": [\"art_painting\", \"photo\", \"sketch\", \"cartoon\"],\n",
    "        \"transforms\": [\n",
    "                {\n",
    "                    \"name\": \"ToTensor\",\n",
    "                    \"kwargs\": {}\n",
    "                },\n",
    "                {  \n",
    "                    \"name\": \"Normalize\",\n",
    "                    \"kwargs\": {\n",
    "                        \"mean\": [0.5, 0.5, 0.5],\n",
    "                        \"std\": [0.5, 0.5, 0.5]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset[\"kwargs\"][\"transforms\"] = torchvision.transforms.Compose(\n",
    "    [init_object(torchvision.transforms, obj_config)\n",
    "        for obj_config in dataset[\"kwargs\"][\"transforms\"]]\n",
    ")\n",
    "\n",
    "domains = dataset[\"kwargs\"][\"domain_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(test_domain):\n",
    "    test_dataset = deepcopy(dataset)\n",
    "    test_dataset[\"kwargs\"][\"dataset_type\"] = [\"train\", \"test\"]\n",
    "\n",
    "  \n",
    "    test_dataset[\"kwargs\"][\"domain_list\"] = [domains[test_domain]]\n",
    "    test_dataset[\"kwargs\"][\"augmentations\"] = None\n",
    "\n",
    "   \n",
    "    test_dataset = PACS(**test_dataset[\"kwargs\"])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    return test_loader, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice\n",
    "test_loader, test_dataset = create_loader(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_iters_loader(loader: torch.utils.data.DataLoader, num_iters: int):\n",
    "    iter = 0\n",
    "    while iter < num_iters:\n",
    "        for batch in loader:\n",
    "            yield batch \n",
    "            iter += 1\n",
    "            if iter == num_iters:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for batch in num_iters_loader(test_loader, 500):\n",
    "    num += batch[\"image\"].shape[0]\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 2048.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds.shape[0], inds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
